{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CTJuaUTzTqJf"
      },
      "outputs": [],
      "source": [
        "#Data analysis\n",
        "# Data Analysis. Data Analysis is the process of systematically applying statistical and/or logical techniques to describe and illustrate, condense and recap, and evaluate data.\n",
        "# Data aes of Data are as follows\n",
        "# Tabular Data > csu > xls > url > jsort > aps.\n",
        "# Tabulation is a systematic and logical representation of numeric data in rows and columns to facilitate comparison and statistical analysis. It facilitates comparison by bringing related information close to each other and helps in statistical analysis and interpretation.\n",
        "# Categorical variables represent types of data which may be divided into groups. Examples of categorical variables are race, sex, age group, and educational level.\n",
        "# Continuous data is a type of numerical data that refers to the unspecified number of possible measurements between two realistic points. These numbers are not always clean and tidy like those in discrete data, as they're usually collected from precise measurements.\n",
        "\n",
        "#Domain Knowledge\n",
        "# Domain knowledge gives data analysts the necessary context for turning data into actionable insights. It allows them to correctly interpret data, identify meaningful patterns, and provide recommendations addressing real-world business issues.\n",
        "\n",
        "# Grouped data are data formed by aggregating individual observations of a variable into groups, so that a frequency distribution of these groups serves as a convenient means of summarizing or analyzing the data.\n",
        "# A common task seen in data analysis is to group the data into different categories and then perform calculations on the resultant groups of data.\n",
        "nalysis is the process of cleaning, changing, and processing raw data and extracting actionable, relevant information that helps businesses make informed decisions.\n",
        "# Primary data are those that are collected for the first time. Secondary data refer to those data that have already been collected by some other person. These are original because these are collected by the investigator for the first time.\n",
        "\n",
        "#Typ\n",
        "#Mean,Median,Mode\n",
        "# The mean (average) of a data set is found by adding all numbers in the data set and then dividing by the number of values in the set. The median is the middle value when a data set is ordered from least to greatest. The mode is the number that occurs most often in a data set.\n",
        "# Outliers are data points that deviate significantly from the rest of the data, while missing values are data points that are absent or incomplete. Here's how to handle outliers and missing values in your instrumentation data using some common methods and tools.\n",
        "\n",
        "#Analysing the data\n",
        "# Data Analysis is the process of systematically applying statistical and/or logical techniques to describe and illustrate, condense and recap, and evaluate data.\n",
        "\n",
        "#Cardinality\n",
        "# Cardinal utility analysis is the oldest theory of demand which provides an explanation of consumer's demand for a product and derives the law of demand which establishes an inverse relationship between price and quantity demanded of a product.\n",
        "# Data cardinality refers to the uniqueness of the values contained in a database column. If most of the values are distinct, then it is considered to have high cardinality. If the column contains mostly repeated values, that makes it a low cardinality column.\n",
        "\n",
        "#Pictorial representation\n",
        "# Among different type of data representations possible, the representation which consists of pictures i.e A pictorial representation of data is called pictograph.\n",
        "# The pictograph is a method to represent the data using images. Each image in the pictograph represents certain things. In other words, pictographs define the frequency of the data using images or symbols, which are relevant to the data.\n",
        "\n",
        "#Statistical representation\n",
        "# A statistical representation of data is a graphical way of representing data in statistics. It helps to understand the distribution of data and to identify any patterns. There are different types of graphs that can be used, depending on the type of data being represented.\n",
        "\n",
        "#Univariant analysis\n",
        "# Univariate analysis is the simplest form of analyzing data. Uni means \"one\", so the data has only one variable (univariate). Univariate data requires to analyze each variable separately. Data is gathered for the purpose of answering a question, or more specifically, a research question.\n",
        "\n",
        "#Bivariate data\n",
        "#This type of data involves two different variables. The analysis of this type of data deals with causes and relationships and the analysis is done to find out the relationship among the two variables. Example of bivariate data can be temperature and ice cream sales in summer season.\n",
        "\n",
        "#Multivariate data\n",
        "#When the data involves three or more variables, it is categorized under multivariate.\n",
        "\n",
        "#Relative Measure of Dispersion\n",
        "#The relative measures of dispersion are used to compare the distribution of two or more data sets. This measure compares values without units. Common relative dispersion methods include: Co-efficient of Range.\n",
        "\n",
        "#Variance\n",
        "#In statistics, variance measures variability from the average or mean. It is calculated by taking the differences between each number in the data set and the mean, then squaring the differences to make them positive, and finally dividing the sum of the squares by the number of values in the data set.\n",
        "\n",
        "#Kernel Entity Estimation\n",
        "# Kernel density estimation is a technique for estimation of probability density function that is a must-have enabling the user to better analyse the studied probability distribution than when using a traditional histogram.\n",
        "\n",
        "#Skewness\n",
        "# Skewness is a measurement of the distortion of symmetrical distribution or asymmetry in a data set. Skewness is demonstrated on a bell curve when data points are not distributed symmetrically to the left and right sides of the median on a bell curve.\n",
        "\n",
        "#Inferential Statistics\n",
        "# Inferential statistics is the practice of using sampled data to draw conclusions or make predictions about a larger sample data sample or population.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "34ujOhjpVpfa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}